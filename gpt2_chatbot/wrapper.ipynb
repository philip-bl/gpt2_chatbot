{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_sampler import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hello my name is Ivan, nice to meet you\", \n",
    "            \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\", \n",
    "            \"Oh, hi, Mark. What a story you just told me...\"]\n",
    "\n",
    "\n",
    "def wrap_message_list(m_list, insert_intro=True, wrap_type='name', check_end_punct=True):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    m_list : list\n",
    "        list of messages in chatbot log \n",
    "    insert_intro : bool, optional\n",
    "        whether should it insert the intro about the conversation\n",
    "    wrap_type : string, optional\n",
    "        type of conditioning to use ('name', 'name-in-par', 'dash', 'number') \n",
    "    check_end_punct : bool, optional\n",
    "        whether should it check the last symbol of message to have the period etc\n",
    "    '''\n",
    "    output = \"\"\n",
    "    types = {'name': ('Alice: ', 'Bob: '),\n",
    "            'name-in-par': ('[Alice]: ', '[Bob]: '),\n",
    "            'dash': ('-', '-'),\n",
    "            'number': ('1: ', '2: ')}\n",
    "    valid_ending = ['.', '!', '?', '\\'']\n",
    "    \n",
    "    assert wrap_type in types, \"Unknown wrapping\"\n",
    "    \n",
    "    if(insert_intro):\n",
    "        output += \"This is the conversation between 2 people.\\n\"\n",
    "        \n",
    "    for i, msg in enumerate(m_list):\n",
    "        output += '\\n'        \n",
    "        output += types[wrap_type][i%2]\n",
    "        output += msg\n",
    "        if((check_end_punct) and (msg[-1] not in valid_ending)):\n",
    "            output += '.'\n",
    "            \n",
    "    output += '\\n'\n",
    "    output += types[wrap_type][(i+1)%2]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is Ivan, nice to meet you',\n",
       " \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\",\n",
       " 'Oh, hi, Mark. What a story you just told me...']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the conversation between 2 people.\n",
      "\n",
      "Alice: Hello my name is Ivan, nice to meet you.\n",
      "Bob: Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\n",
      "Alice: Oh, hi, Mark. What a story you just told me...\n",
      "Bob: \n"
     ]
    }
   ],
   "source": [
    "text4test = wrap_message_list(messages, wrap_type='name')\n",
    "print(text4test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(seed=0, model_name_or_path='gpt2'):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "    # model.load_state_dict(torch.load( ... )) uncommet it to make it work....\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, enc, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_text, *model_params, length=-1, top_k=0, temperature=1.0):\n",
    "    model, enc, device = model_params\n",
    "    if length == -1:\n",
    "        length = model.config.n_ctx // 2\n",
    "    elif length > model.config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "        \n",
    "    context_tokens = []\n",
    "    context_tokens = enc.encode(input_text)\n",
    "\n",
    "    out = sample_sequence(\n",
    "        model=model, length=length,\n",
    "        context=context_tokens,\n",
    "        start_token=None,\n",
    "        batch_size=1,\n",
    "        temperature=temperature, top_k=top_k, device=device)\n",
    "\n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    output_text = enc.decode(out[0])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_answer(user_input, prev_msgs, *model_params, **wrap_params):\n",
    "    prev_msgs.append(user_input)\n",
    "    input_text = wrap_message_list(prev_msgs, **wrap_params)\n",
    "    \n",
    "    sampled_answer = model_forward(input_text, *model_params)\n",
    "    print(\"All sampled:\\n\", sampled_answer, \"\\n\\n\")\n",
    "    answer = sampled_answer.split('\\n')[0]\n",
    "    prev_msgs.append(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, enc, device = init_model(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:26<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sampled:\n",
      " ????? Right now? ?????\n",
      "\n",
      "Alice: ????????\n",
      "\n",
      "Webcam Kodak will come out on Feb. 27. Counter's cameras will be shown, but it seems Congress is going to run the numbers.<|endoftext|>University of Texas President Dawn Canady: Woman kissed NHL star\n",
      "\n",
      "University of Texas President Dawn Canady: Bureau of Stadium Services no longer includes cross references to nude photos in bus advertisements, commentator says\n",
      "\n",
      "Election Consultancy group is now customers of one Miss San Antonio. The news burns right in the wrong wing of the computer somewhere. Please fix this. Democrat Naheed Nenshi, who owned an alpacas dealership in Houston Thursday night, fears a possible newspaper article listing street fighting sets off a news storm this week over \"distracted drivers\" using cross references in advertising.\n",
      "\n",
      "Yehuda said physiotherapist David Serutter of Saint Mary's Athletic Club paid $5 a day for Lat chefs Evangeline Bellingham and John Sacrange. Serutter said Thursday that flights \"remain no more five years old,\" and only five jobs will open next week.\n",
      "\n",
      "Serutter said he tried buying a single round winter horse deer volley from Saint Mary's the other night, but he thought everyone was Ohio wolves. He also hopes Ninety-Nine is, at the very least, worth paying $500 for.\n",
      "\n",
      "It boiling in the Big 12 is a new shade of pink and \"maybe a few concrete plans for this if it starts construction on a new stadium,\" Famin said, suggesting a three-fracked stadium where games next season will go to a clear playoff berth, instead of the original segregated, white-washed one.\n",
      "\n",
      "\"If these kind of things happen, boom! The economy will recover and we will have 95,000,\" said Stockton Hill, said at Nationals Days of opportunity in College Park.\n",
      "\n",
      "\"Anytime something's antitrust and downstream a lot of things that enrich the sports business, or any substantial gain to sports is made, those kinds of factors generate trade disputes. It does that for a long time now,\" Rep. Tom Davis said. \"It's just a death spiral in knowledge of industry in a few difficult areas.\"\n",
      "\n",
      "Stockton is led by Dr. Toshmari Khan of Smarter Games Consultancy, a giant contractor who relies on fancy pacemaker miniaturizing for jet-hip airliner flights. His Foist will take 20,000 investments, forcing workers to gouge out numbers and charge double fares to cover overtime \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'????? Right now? ?????'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Hello, my name is Ivan, nice to meet you!\", messages, model, enc, device, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, my name is Ivan, nice to meet you!',\n",
       " '????? Right now? ?????',\n",
       " 'Hello, my name is Ivan, nice to meet you!',\n",
       " '????? Right now? ?????']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
