{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_sampler import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hello my name is Ivan, nice to meet you\", \n",
    "            \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\", \n",
    "            \"Oh, hi, Mark. What a story you just told me...\"]\n",
    "\n",
    "\n",
    "def wrap_message_list(m_list, insert_intro=True, wrap_type='name', check_end_punct=True):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    m_list : list\n",
    "        list of messages in chatbot log \n",
    "    insert_intro : bool, optional\n",
    "        whether should it insert the intro about the conversation\n",
    "    wrap_type : string, optional\n",
    "        type of conditioning to use ('name', 'name-in-par', 'dash', 'number') \n",
    "    check_end_punct : bool, optional\n",
    "        whether should it check the last symbol of message to have the period etc\n",
    "    '''\n",
    "    output = \"\"\n",
    "    types = {'name': ('Alice: ', 'Bob: '),\n",
    "            'name-in-par': ('[Alice]: ', '[Bob]: '),\n",
    "            'dash': ('-', '-'),\n",
    "            'number': ('1: ', '2: ')}\n",
    "    valid_ending = ['.', '!', '?', '\\'']\n",
    "    \n",
    "    assert wrap_type in types, \"Unknown wrapping\"\n",
    "    \n",
    "    if(insert_intro):\n",
    "        output += \"<|endoftext|>\"#This is the conversation between 2 people.\"\n",
    "        \n",
    "    for i, msg in enumerate(m_list):\n",
    "        output += types[wrap_type][i%2]\n",
    "        output += msg\n",
    "        if((check_end_punct) and (msg[-1] not in valid_ending)):\n",
    "            output += '.'\n",
    "        output += '\\n'        \n",
    "            \n",
    "    #output += '\\n'\n",
    "    output += types[wrap_type][(i+1)%2]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is Ivan, nice to meet you',\n",
       " \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\",\n",
       " 'Oh, hi, Mark. What a story you just told me...']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Alice: Hello my name is Ivan, nice to meet you.\n",
      "Bob: Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\n",
      "Alice: Oh, hi, Mark. What a story you just told me...\n",
      "Bob: \n"
     ]
    }
   ],
   "source": [
    "text4test = wrap_message_list(messages, wrap_type='name')\n",
    "print(text4test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(seed=0, model_name_or_path='gpt2'):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(\"../gpt2_model_3200.pth\"))\n",
    "    model = model.module\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, enc, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_text, *model_params, length=128, top_k=10, temperature=1.0):\n",
    "    model, enc, device = model_params\n",
    "    if length == -1:\n",
    "        length = model.config.n_ctx // 2\n",
    "    elif length > model.config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "        \n",
    "    context_tokens = []\n",
    "    context_tokens = enc.encode(input_text)\n",
    "    context_tokens = [50256, 220] + context_tokens\n",
    "    print(\"Input tokens\")\n",
    "    print(context_tokens)\n",
    "    \n",
    "    out = sample_sequence(\n",
    "        model=model, length=length,\n",
    "        context=context_tokens,\n",
    "        start_token=None,\n",
    "        batch_size=1,\n",
    "        temperature=temperature, top_k=top_k, device=device)\n",
    "\n",
    "    print(\"Out Tokens\") \n",
    "    print(out)    \n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    output_text = enc.decode(out[0])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_answer(user_input, prev_msgs, *model_params, **wrap_params):\n",
    "    prev_msgs.append(user_input)\n",
    "    input_text = wrap_message_list(prev_msgs, **wrap_params)\n",
    "    print(\"Model input:\\n\")\n",
    "    print(input_text)\n",
    "    sampled_answer = model_forward(input_text, *model_params)\n",
    "    print(\"All sampled:\\n\")\n",
    "    print(sampled_answer) \n",
    "    print(\"\\n\\n\")\n",
    "    answer = sampled_answer.split('\\n')[0] ### If <end of text. -> send ...\n",
    "    answer = answer.replace(u'\\xa0', u'') ### FIX THIS\n",
    "    prev_msgs.append(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, enc, device = init_model(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input:\n",
      "\n",
      "Alice: Hi! Do you have any hobbies.\n",
      "Bob: \n",
      "Input tokens\n",
      "[50256, 220, 44484, 25, 15902, 0, 2141, 345, 423, 597, 45578, 13, 198, 18861, 25, 220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 128/128 [00:07<00:00, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out Tokens\n",
      "tensor([[50256,   220, 44484,    25, 15902,     0,  2141,   345,   423,   597,\n",
      "         45578,    13,   198, 18861,    25,   220,  9805,   198, 44484,    25,\n",
      "         14373,   198, 44484,    25,  4162,   389,   345,  4737,   703, 12248,\n",
      "          6530,    30,   198, 18861,    25,  3966,  1392,   340, 12248,   198,\n",
      "         44484,    25,  9425,   198, 18861,    25,  1867,   338,   534,  1438,\n",
      "          5633,   220, 13300,   530,   640,   198, 44484,    25, 15929,   198,\n",
      "         18861,    25,   679,   258,   314,  1101,  4422, 14373,   198,    27,\n",
      "            91,   437,  1659,  5239,    91,    29, 44484,    25, 23105,   198,\n",
      "         44484,    25,   285,   393,   277,   198, 18861,    25, 14690,   198,\n",
      "         18861,    25,   355,    30,   198,    27,    91,   437,  1659,  5239,\n",
      "            91,    29, 44484,    25, 17207,   198, 44484,    25,   355,    75,\n",
      "           198, 18861,    25,   355,    75,   198, 18861,    25,  2956,    30,\n",
      "            77,   198, 18861,    25,  1596,    76,   773,   544,   198,    27,\n",
      "            91,   437,  1659,  5239,    91,    29, 44484,    25, 23105,   198,\n",
      "         44484,    25,   355,    75]], device='cuda:0')\n",
      "All sampled:\n",
      "\n",
      "????\n",
      "Alice: :)\n",
      "Alice: Why are you asking how?! Name?\n",
      "Bob: Oh got it?!\n",
      "Alice: Yeah\n",
      "Bob: What's your name ? Maybe one time\n",
      "Alice: Neil\n",
      "Bob: Hehe I'm Alex :)\n",
      "<|endoftext|>Alice: hi\n",
      "Alice: m or f\n",
      "Bob: Hey\n",
      "Bob: as?\n",
      "<|endoftext|>Alice: hey\n",
      "Alice: asl\n",
      "Bob: asl\n",
      "Bob: ur?n\n",
      "Bob: 17m india\n",
      "<|endoftext|>Alice: hi\n",
      "Alice: asl\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'????'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Hi! Do you have any hobbies\", messages, model, enc, device, insert_intro=False, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi! Do you have any hobbies', '???????????????']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'????'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode([9805])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
