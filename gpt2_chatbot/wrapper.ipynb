{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_sampler import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hello my name is Ivan, nice to meet you\", \n",
    "            \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\", \n",
    "            \"Oh, hi, Mark. What a story you just told me...\"]\n",
    "\n",
    "\n",
    "def wrap_message_list(m_list, insert_intro=True, wrap_type='name', check_end_punct=True):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    m_list : list\n",
    "        list of messages in chatbot log \n",
    "    insert_intro : bool, optional\n",
    "        whether should it insert the intro about the conversation\n",
    "    wrap_type : string, optional\n",
    "        type of conditioning to use ('name', 'name-in-par', 'dash', 'number') \n",
    "    check_end_punct : bool, optional\n",
    "        whether should it check the last symbol of message to have the period etc\n",
    "    '''\n",
    "    output = \"\"\n",
    "    types = {'name': ('Alice:', 'Bob:'),\n",
    "            #'name-in-par': ('[Alice]:', '[Bob]:'),\n",
    "            'dash': ('-', '-'),\n",
    "            'number': ('1:', '2:')}\n",
    "    valid_ending = ['.', '!', '?']\n",
    "    \n",
    "    assert wrap_type in types, \"Unknown wrapping\"\n",
    "    \n",
    "    if(insert_intro):\n",
    "        output += \"<|endoftext|>\"\n",
    "        output += \"This is the conversation between 2 people.\\n\"\n",
    "        \n",
    "    for i, msg in enumerate(m_list):\n",
    "        output += types[wrap_type][i%2]\n",
    "        output += ' '\n",
    "        output += msg\n",
    "        if((check_end_punct) and (msg[-1] not in valid_ending)):\n",
    "            output += '.'\n",
    "        output += '\\n'        \n",
    "            \n",
    "    #output += '\\n'\n",
    "    output += types[wrap_type][(i+1)%2]\n",
    "    \n",
    "    if(types[wrap_type][0][-1] == ':'):\n",
    "        conditioning = types[wrap_type][0][:-1]\n",
    "    else:\n",
    "        conditioning = types[wrap_type][0]\n",
    "    \n",
    "    return output, [conditioning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is Ivan, nice to meet you',\n",
       " \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\",\n",
       " 'Oh, hi, Mark. What a story you just told me...']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>This is the conversation between 2 people.\n",
      "Alice: Hello my name is Ivan, nice to meet you.\n",
      "Bob: Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\n",
      "Alice: Oh, hi, Mark. What a story you just told me...\n",
      "Bob:|\n",
      "Stop token: ['Alice']\n"
     ]
    }
   ],
   "source": [
    "text4test, ctoken = wrap_message_list(messages, wrap_type='name')\n",
    "print(text4test+'|')\n",
    "print(\"Stop token:\", ctoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(seed=0, model_name_or_path='gpt2'):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    seed : int\n",
    "        seed number for different ramdomizers\n",
    "    model_name_or_path : string, optional\n",
    "        either model name for existing model or path for trained model\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    \n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_name_or_path))\n",
    "    model = model.module\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, enc, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_text, conditioning, verbose, *model_params, length=128, top_k=10, temperature=1.0):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_text : string\n",
    "        input text for sampling\n",
    "    *model_params : tuple\n",
    "        (model, enc, device) output of 'init_model' function\n",
    "    length : int, optional\n",
    "        length of generated sample I guess (!!not sure!!)\n",
    "    top_k : int, optional\n",
    "        to generate k most probable samples (!!not sure!!)\n",
    "    temperature: float, optional\n",
    "        parameter of sampling algorithm\n",
    "    '''\n",
    "    model, enc, device = model_params\n",
    "    if length == -1:\n",
    "        length = model.config.n_ctx // 2\n",
    "    elif length > model.config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "        \n",
    "    context_tokens = []\n",
    "    context_tokens = enc.encode(input_text)\n",
    "    context_tokens = [50256] + context_tokens\n",
    "    \n",
    "    cond_tokens = []\n",
    "    for token in conditioning:\n",
    "        cond_tokens += enc.encode(token)\n",
    "    \n",
    "    if(verbose):\n",
    "        print('Detected conditioning tokens:')\n",
    "        print(cond_tokens)\n",
    "        print(\"Input tokens:\")\n",
    "        print(context_tokens)\n",
    "    \n",
    "    out = sample_sequence(\n",
    "        model=model, length=length,\n",
    "        context=context_tokens, cond_tokens=cond_tokens,\n",
    "        start_token=None,\n",
    "        batch_size=1,\n",
    "        temperature=temperature, top_k=top_k, device=device)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Out Tokens:\") \n",
    "        print(out)    \n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    output_text = enc.decode(out[0])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(string, delimeters):\n",
    "    sentences = []\n",
    "    prev_end = 0\n",
    "    for i in range(1, len(string)):\n",
    "        if((string[i-1] in delimeters) & (string[i] not in delimeters)):\n",
    "            str_to_append = string[prev_end:i-1].strip() + string[i-1]\n",
    "            sentences.append((str_to_append[0].upper() + str_to_append[1:]).replace('¿', ''))\n",
    "            prev_end = i\n",
    "    str_to_append = string[prev_end:-1].strip() + string[-1]\n",
    "    #print(\"Strting to append:\", '\\''+str_to_append+'\\'')\n",
    "    sentences.append((str_to_append[0].upper() + str_to_append[1:]).replace('¿', ''))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_post_processing(input_quote, max_words):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    input_quote : string\n",
    "        output of model\n",
    "    max_words : integer\n",
    "        maximal number of words (rounded to the end of sentence with last word) to concatenate to phrase\n",
    "    '''\n",
    "    valid_endings = ['.', '!', '?', '¿']\n",
    "    input_quote = input_quote.replace(u'\\xa0', '') # filter out \"\\xa0\" \n",
    "    input_quote = input_quote.replace('\\n', ' ') # filter out \"\\n\" \n",
    "    \n",
    "    first_endoftext = input_quote.find('<|endoftext|>') \n",
    "    if(first_endoftext != -1):\n",
    "        input_quote = input_quote[:first_endoftext] # cut the string when '<|endoftext|>' found'\n",
    "        \n",
    "    first_Alice = input_quote.find('Alice:') \n",
    "    if(first_Alice != -1):\n",
    "        input_quote = input_quote[:first_Alice] # cut the string when 'Alice:' found'\n",
    "\n",
    "    input_quote = input_quote.replace(\"Bob:\", '¿') # filter out \"Bob: \"\n",
    "    print('Partially processed: ')\n",
    "    print(input_quote+'|')\n",
    "    \n",
    "    sentences = split(input_quote.strip(), valid_endings) # sprit remaining string to sentences according to delimiters\n",
    "    \n",
    "    print(sentences)\n",
    "    \n",
    "    sentences = list(filter(None, sentences)) # filter out empty strings\n",
    "    sentences = list(filter(lambda x: x != '¿', sentences))  # filter out empty strings\n",
    "    \n",
    "    print(sentences)\n",
    "\n",
    "    for i, sentence in enumerate(sentences): # add periods where nessecary\n",
    "#         for j in range(len(sentence)-1, 1, -1):\n",
    "#             if((not sentence[j].isalnum()) & (sentence[j-1].isalnum())):\n",
    "#                 left_part = sentence[:j]\n",
    "#                 right_part = sentence[j:]\n",
    "#                 print(\"\\n\\n!!!SPACE BETWEEN PUNCTUATION AND WORDS!!!\")\n",
    "#                 print(\"BEFORE:\")\n",
    "#                 print(left_part, right_part)\n",
    "#                 right_part = right_part.replace(' ', '')\n",
    "#                 print(\"AFTER:\")\n",
    "#                 print(left_part + right_part)\n",
    "#                 sentences[i] = left_part + right_part\n",
    "#                 break\n",
    "        \n",
    "        if(sentence[-1] not in valid_endings):\n",
    "            sentences[i] += '.'\n",
    "    \n",
    "    word_counts = [len(s.split(' ')) for s in sentences]\n",
    "    word_cum_counts = np.cumsum(np.array(word_counts)) / max_words\n",
    "    sentences_to_pass = np.sum(word_cum_counts < 1.0) + 1\n",
    "    \n",
    "    return \" \".join(sentences[:sentences_to_pass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partially processed: \n",
      "Kek .. ?? ¿ Kek, definetely kek|\n",
      "['Kek ..', '??', '', 'Kek, definetely kek']\n",
      "['Kek ..', '??', 'Kek, definetely kek']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kek .. ?? Kek, definetely kek.'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Kek .. ?? Bob: Kek, definetely kek\"\n",
    "output_post_processing(s, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_answer(user_input, prev_msgs, max_words, verbose=False, *model_params, **wrap_params):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    user_input : string\n",
    "        user's message\n",
    "    prev_msgs : list\n",
    "        list of previous messages in conversation\n",
    "    max_words : integer\n",
    "        number of words to generate (rounded to the end of last sentence)\n",
    "    *model_params : tuple\n",
    "        (model, enc, device) output of 'init_model' function\n",
    "    **wrap_parameters : dict\n",
    "        parametrs for 'wrap_message_list' function like `wrap_type`    \n",
    "    '''\n",
    "    prev_msgs.append(user_input)\n",
    "    input_text, conditioning = wrap_message_list(prev_msgs, **wrap_params)\n",
    "    if(verbose):\n",
    "        print(\"Model input:\\n\")\n",
    "        print(input_text)\n",
    "    sampled_answer = model_forward(input_text, conditioning, verbose, *model_params)\n",
    "    if(verbose):\n",
    "        print(\"All sampled:\\n\")\n",
    "        print(sampled_answer) \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "    answer = output_post_processing(sampled_answer, max_words)\n",
    "    \n",
    "    prev_msgs.append(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, enc, device = init_model(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input:\n",
      "\n",
      "Alice: Hi! Do you have any hobbies?\n",
      "Bob:\n",
      "Detected conditioning tokens:\n",
      "[44484]\n",
      "Input tokens:\n",
      "[50256, 220, 44484, 25, 15902, 0, 2141, 345, 423, 597, 45578, 30, 198, 18861, 25]\n",
      "Out Tokens:\n",
      "tensor([[50256,   220, 44484,    25, 15902,     0,  2141,   345,   423,   597,\n",
      "         45578,    30,   198, 18861,    25,  1312, 17666,   466,   881,   290,\n",
      "          1312,   588,   284,  1561,   546,   616, 45578,   198, 18861,    25,\n",
      "          1312,   711, 10047,   198, 18861,    25,  1312,   423,   645, 45578,\n",
      "           198]], device='cuda:0')\n",
      "All sampled:\n",
      "\n",
      " i dont do much and i like to talk about my hobbies\n",
      "Bob: i play guitar\n",
      "Bob: i have no hobbies\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Partially processed: \n",
      " i dont do much and i like to talk about my hobbies ¿ i play guitar ¿ i have no hobbies |\n",
      "['I dont do much and i like to talk about my hobbies', 'I play guitar', 'I have no hobbies']\n",
      "['I dont do much and i like to talk about my hobbies', 'I play guitar', 'I have no hobbies']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I dont do much and i like to talk about my hobbies. I play guitar. I have no hobbies.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Hi! Do you have any hobbies?\", messages, 30, True, \n",
    "               model, enc, device, \n",
    "               insert_intro=False, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input:\n",
      "\n",
      "Alice: Hi! Do you have any hobbies?\n",
      "Bob: I dont do much and i like to talk about my hobbies. I play guitar. I have no hobbies.\n",
      "Alice: Me too. Do you like play computer games?\n",
      "Bob:\n",
      "Detected conditioning tokens:\n",
      "[44484]\n",
      "Input tokens:\n",
      "[50256, 220, 44484, 25, 15902, 0, 2141, 345, 423, 597, 45578, 30, 198, 18861, 25, 314, 17666, 466, 881, 290, 1312, 588, 284, 1561, 546, 616, 45578, 13, 314, 711, 10047, 13, 314, 423, 645, 45578, 13, 198, 44484, 25, 2185, 1165, 13, 2141, 345, 588, 711, 3644, 1830, 30, 198, 18861, 25]\n",
      "Out Tokens:\n",
      "tensor([[50256,   220, 44484,    25, 15902,     0,  2141,   345,   423,   597,\n",
      "         45578,    30,   198, 18861,    25,   314, 17666,   466,   881,   290,\n",
      "          1312,   588,   284,  1561,   546,   616, 45578,    13,   314,   711,\n",
      "         10047,    13,   314,   423,   645, 45578,    13,   198, 44484,    25,\n",
      "          2185,  1165,    13,  2141,   345,   588,   711,  3644,  1830,    30,\n",
      "           198, 18861,    25,  9425,     0,   198]], device='cuda:0')\n",
      "All sampled:\n",
      "\n",
      " Yeah!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Partially processed: \n",
      " Yeah! |\n",
      "['Yeah!']\n",
      "['Yeah!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yeah!'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Me too. Do you like play computer games?\", messages, 30, True, \n",
    "               model, enc, device, \n",
    "               insert_intro=False, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi! Do you have any hobbies?',\n",
       " 'I dont do much and i like to talk about my hobbies. I play guitar. I have no hobbies.',\n",
       " 'Me too. Do you like play computer games?',\n",
       " 'Yeah!']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
