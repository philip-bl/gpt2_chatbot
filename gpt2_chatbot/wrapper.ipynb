{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_sampler import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\"Hello my name is Ivan, nice to meet you\", \n",
    "            \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\", \n",
    "            \"Oh, hi, Mark. What a story you just told me...\"]\n",
    "\n",
    "\n",
    "def wrap_message_list(m_list, insert_intro=True, wrap_type='name', check_end_punct=True):\n",
    "    '''\n",
    "    Parameters:\n",
    "    ----------\n",
    "    m_list : list\n",
    "        list of messages in chatbot log \n",
    "    insert_intro : bool, optional\n",
    "        whether should it insert the intro about the conversation\n",
    "    wrap_type : string, optional\n",
    "        type of conditioning to use ('name', 'name-in-par', 'dash', 'number') \n",
    "    check_end_punct : bool, optional\n",
    "        whether should it check the last symbol of message to have the period etc\n",
    "    '''\n",
    "    output = \"\"\n",
    "    types = {'name': ('Alice:', 'Bob:'),\n",
    "            #'name-in-par': ('[Alice]:', '[Bob]:'),\n",
    "            'dash': ('-', '-'),\n",
    "            'number': ('1:', '2:')}\n",
    "    valid_ending = ['.', '!', '?', '\\'', '\\\"']\n",
    "    \n",
    "    assert wrap_type in types, \"Unknown wrapping\"\n",
    "    \n",
    "    if(insert_intro):\n",
    "        output += \"<|endoftext|>\"#This is the conversation between 2 people.\"\n",
    "        \n",
    "    for i, msg in enumerate(m_list):\n",
    "        output += types[wrap_type][i%2]\n",
    "        output += ' '\n",
    "        output += msg\n",
    "        if((check_end_punct) and (msg[-1] not in valid_ending)):\n",
    "            output += '.'\n",
    "        output += '\\n'        \n",
    "            \n",
    "    #output += '\\n'\n",
    "    output += types[wrap_type][(i+1)%2]\n",
    "    \n",
    "    conditioning = []\n",
    "    if(types[wrap_type][0][-1] == ':'):\n",
    "        conditioning.append(types[wrap_type][0][:-1])\n",
    "        conditioning.append(types[wrap_type][1][:-1])\n",
    "    else:\n",
    "        conditioning = list(types[wrap_type])\n",
    "    \n",
    "    return output, conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello my name is Ivan, nice to meet you',\n",
       " \"Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\",\n",
       " 'Oh, hi, Mark. What a story you just told me...']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>1: Hello my name is Ivan, nice to meet you.\n",
      "2: Hello, Ivan, nive to meet you too. I'm a YetAnotherChatbot.\n",
      "1: Oh, hi, Mark. What a story you just told me...\n",
      "2:|\n"
     ]
    }
   ],
   "source": [
    "text4test, ctokens = wrap_message_list(messages, wrap_type='number')\n",
    "print(text4test+'|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(seed=0, model_name_or_path='gpt2'):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    enc = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name_or_path)\n",
    "    \n",
    "    model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(\"../gpt2_model_3200.pth\"))\n",
    "    model = model.module\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, enc, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward(input_text, conditioning, verbose, *model_params, length=128, top_k=10, temperature=1.0):\n",
    "    model, enc, device = model_params\n",
    "    if length == -1:\n",
    "        length = model.config.n_ctx // 2\n",
    "    elif length > model.config.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % model.config.n_ctx)\n",
    "        \n",
    "    context_tokens = []\n",
    "    context_tokens = enc.encode(input_text)\n",
    "    context_tokens = [50256, 220] + context_tokens\n",
    "    \n",
    "    cond_tokens = []\n",
    "    for token in conditioning:\n",
    "        cond_tokens += enc.encode(token)\n",
    "    \n",
    "    if(verbose):\n",
    "        print('Detected conditioning tokens:')\n",
    "        print(cond_tokens)\n",
    "        print(\"Input tokens:\")\n",
    "        print(context_tokens)\n",
    "    \n",
    "    out = sample_sequence(\n",
    "        model=model, length=length,\n",
    "        context=context_tokens, cond_tokens=cond_tokens,\n",
    "        start_token=None,\n",
    "        batch_size=1,\n",
    "        temperature=temperature, top_k=top_k, device=device)\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Out Tokens:\") \n",
    "        print(out)    \n",
    "    out = out[:, len(context_tokens):].tolist()\n",
    "    output_text = enc.decode(out[0])\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_answer(user_input, prev_msgs, verbose=False, *model_params, **wrap_params):\n",
    "    prev_msgs.append(user_input)\n",
    "    input_text, conditioning = wrap_message_list(prev_msgs, **wrap_params)\n",
    "    if(verbose):\n",
    "        print(\"Model input:\\n\")\n",
    "        print(input_text)\n",
    "    sampled_answer = model_forward(input_text, conditioning, verbose, *model_params)\n",
    "    if(verbose):\n",
    "        print(\"All sampled:\\n\")\n",
    "        print(sampled_answer) \n",
    "        print(\"\\n\\n\")\n",
    "    answer = sampled_answer.split('\\n')[0] ### If <end of text> -> send ...\n",
    "    answer = answer.replace(u'\\xa0', '') ### FIX THIS\n",
    "    \n",
    "    if(answer[0] == ' '):\n",
    "        answer = answer[1:]\n",
    "    \n",
    "    prev_msgs.append(answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, enc, device = init_model(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Hi! Do you have any hobbies?\", messages, False, model, enc, device, insert_intro=False, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really just bored.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "produce_answer(\"Me too. Do you like play computer games?\", messages, False, model, enc, device, insert_intro=False, wrap_type='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi! Do you have any hobbies?',\n",
       " 'No',\n",
       " 'Me too. Do you like play computer games?',\n",
       " \"I'm really just bored.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
